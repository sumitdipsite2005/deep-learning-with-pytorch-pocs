{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aabafca-8129-4943-b865-d5e897637253",
   "metadata": {},
   "source": [
    "![image](car.jpeg)\n",
    "\n",
    "**Car-ing is sharing**, an auto dealership company for car sales and rental, is taking their services to the next level thanks to **Large Language Models (LLMs)**.\n",
    "\n",
    "As their newly recruited AI and NLP developer, you've been asked to prototype a chatbot app with multiple functionalities that not only assist customers but also provide support to human agents in the company.\n",
    "\n",
    "The solution should receive textual prompts and use a variety of pre-trained Hugging Face LLMs to respond to a series of tasks, e.g. classifying the sentiment in a carâ€™s text review, answering a customer question, summarizing or translating text, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c0004-e8c7-4539-967d-0c32167ae540",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "In order to complete the project you may wish to install some Hugging Face libraries such as `transformers` and `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5325a4c0-ceb3-4b66-acd2-5eadcefe3a63",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 17400,
    "lastExecutedAt": 1741278914183,
    "lastExecutedByKernel": "fea07ce4-6405-40bb-98ad-5ba970d3605c",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install transformers\n!pip install evaluate==0.4.0\n!pip install datasets==2.10.0\n!pip install sentencepiece==0.1.97\n\nfrom transformers import logging\nlogging.set_verbosity(logging.WARNING)",
    "outputsMetadata": {
     "0": {
      "height": 553,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install evaluate==0.4.0\n",
    "# !pip install datasets==2.10.0\n",
    "# #!pip install sentencepiece==0.1.97\n",
    "# !pip install sentencepiece\n",
    "\n",
    "# from transformers import logging\n",
    "# logging.set_verbosity(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d60b65b6-6b33-4128-b76e-f8b86aa31070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandoc\n",
    "#!pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892975b9-1fcf-4607-9f0a-8114f26ee798",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6638,
    "lastExecutedAt": 1741278920823,
    "lastExecutedByKernel": "fea07ce4-6405-40bb-98ad-5ba970d3605c",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport torch\nimport evaluate\nfrom datasets import Dataset\nfrom transformers import pipeline, AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b280f-4c2c-4c0b-bbea-a0b3417d9d26",
   "metadata": {},
   "source": [
    "## **LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41aa5237-0e45-4388-8404-11a620981a54",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 245,
      "tableState": {
       "quickFilterText": ""
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am very satisfied with my 2014 Nissan NV SL....</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The car is fine. It's a bit loud and not very ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My first foreign car. Love it, I would buy ano...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've come across numerous reviews praising the...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been dreaming of owning an SUV for quite ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review     Class\n",
       "0  I am very satisfied with my 2014 Nissan NV SL....  POSITIVE\n",
       "1  The car is fine. It's a bit loud and not very ...  NEGATIVE\n",
       "2  My first foreign car. Love it, I would buy ano...  POSITIVE\n",
       "3  I've come across numerous reviews praising the...  NEGATIVE\n",
       "4  I've been dreaming of owning an SUV for quite ...  POSITIVE"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_review = pd.read_csv(\"data/car_reviews.csv\", sep=\";\")\n",
    "car_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84104904-2a68-4a1a-a399-a3b04bc2f248",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1741278920921,
    "lastExecutedByKernel": "fea07ce4-6405-40bb-98ad-5ba970d3605c",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Convert Pandas DataFrame to Hugging Face Dataset\ntest_data = Dataset.from_pandas(car_review)\ntest_data",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 462,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Review', 'Class'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Hugging Face Dataset\n",
    "test_data = Dataset.from_pandas(car_review)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ca4c8e-cd28-47c6-b2f3-b0f56cef7842",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1741279205413,
    "lastExecutedByKernel": "fea07ce4-6405-40bb-98ad-5ba970d3605c",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# from datacamp solution\n# Put the car reviews and their associated sentiment labels in two lists\nreviews_dc = car_review['Review'].tolist()\nreal_labels_dc = car_review['Class'].tolist()\nreal_labels_dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datacamp solution\n",
    "# Put the car reviews and their associated sentiment labels in two lists\n",
    "reviews_dc = car_review['Review'].tolist()\n",
    "real_labels_dc = car_review['Class'].tolist()\n",
    "real_labels_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d6d31-a7d5-4bbe-bc4c-6d16fde59811",
   "metadata": {},
   "source": [
    "## **SENTIMENT ANALYSIS MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c408ca-5e0b-47b0-97a9-fb43d76c3538",
   "metadata": {},
   "source": [
    "### USING PIPELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7db74116-5a56-4b29-a1c6-5f0a6f3312ad",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1442,
    "lastExecutedAt": 1741279628868,
    "lastExecutedByKernel": "fea07ce4-6405-40bb-98ad-5ba970d3605c",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the sentiment-analysis pipeline\nclassifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\n# Run predictions\npredicted_labels = classifier(test_data['Review'])\n\nprint(predicted_labels)\n\n# Extract predicted labels\npredictions = [1 if pred['label'] == 'POSITIVE' else 0 for pred in predicted_labels]\n\n# Print predictions\nprint(predictions)",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     },
     "1": {
      "height": 101,
      "type": "stream"
     },
     "4": {
      "height": 80,
      "type": "stream"
     },
     "5": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9293984174728394}, {'label': 'POSITIVE', 'score': 0.8654274344444275}, {'label': 'POSITIVE', 'score': 0.9994640946388245}, {'label': 'NEGATIVE', 'score': 0.9935314059257507}, {'label': 'POSITIVE', 'score': 0.9986565113067627}]\n",
      "[1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# My solution using pipeline\n",
    "\n",
    "# Load the sentiment-analysis pipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Run predictions\n",
    "predicted_labels = classifier(test_data['Review'])\n",
    "\n",
    "print(predicted_labels)\n",
    "\n",
    "# Extract predicted labels\n",
    "predictions = [1 if pred['label'] == 'POSITIVE' else 0 for pred in predicted_labels]\n",
    "\n",
    "# Print predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd03c61e-5d48-4e4a-9d28-11b971e9da13",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1529,
    "lastExecutedAt": 1741279428729,
    "lastExecutedByKernel": "fea07ce4-6405-40bb-98ad-5ba970d3605c",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# datacamp solution using pipeline\n\n# Load a sentiment analysis LLM into a pipeline\nclassifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n\n# Perform inference on the car reviews and display prediction results\npredicted_labels_dc = classifier(reviews_dc)\nfor review_dc, prediction_dc, label_dc in zip(reviews_dc, predicted_labels_dc, real_labels_dc):\n    print(f\"Review: {review_dc}\\nActual Sentiment: {label_dc}\\nPredicted Sentiment: {prediction_dc['label']} (Confidence: {prediction_dc['score']:.4f})\\n\")\n",
    "outputsMetadata": {
     "0": {
      "height": 462,
      "type": "stream"
     },
     "4": {
      "height": 80,
      "type": "stream"
     },
     "5": {
      "height": 462,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9294)\n",
      "\n",
      "Review: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.8654)\n",
      "\n",
      "Review: My first foreign car. Love it, I would buy another.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9995)\n",
      "\n",
      "Review: I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9935)\n",
      "\n",
      "Review: I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9987)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datacamp solution using pipeline\n",
    "\n",
    "# Load a sentiment analysis LLM into a pipeline\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Perform inference on the car reviews and display prediction results\n",
    "predicted_labels_dc = classifier(reviews_dc)\n",
    "\n",
    "# better way of presenting the prediction output\n",
    "for review_dc, prediction_dc, label_dc in zip(reviews_dc, predicted_labels_dc, real_labels_dc):\n",
    "    print(f\"Review: {review_dc}\\nActual Sentiment: {label_dc}\\nPredicted Sentiment: {prediction_dc['label']} (Confidence: {prediction_dc['score']:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05aa34b-1c9b-4e70-ac5c-a122fa633c91",
   "metadata": {},
   "source": [
    "### USING AUTOMODEL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a85ab0e-688f-49ee-989c-231bf1d57680",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1060,
    "lastExecutedAt": 1741246437833,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")",
    "outputsMetadata": {
     "2": {
      "height": 332,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41d1c150-b7a2-4d99-ba50-69ed309ca196",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1741246437882,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def tokenize_function(text_data):\n    return tokenizer(text_data['Review'], \n                     return_tensors=\"pt\", \n                     padding=True,\n                     truncation=True, \n                     max_length=200)\n\n# Tokenize in batches\ntokenized_test_data = test_data.map(tokenize_function, batched=True)"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e8884437364cb1a9ad9ca6b6d3853d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['Review', 'Class', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# using tokenize function with map()\n",
    "\n",
    "def tokenize_function(text_data):\n",
    "    return tokenizer(text_data['Review'], \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True,\n",
    "                     truncation=True, \n",
    "                     max_length=200)\n",
    "\n",
    "# Tokenize in batches\n",
    "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "print(type(tokenized_test_data))\n",
    "\n",
    "print(tokenized_test_data)\n",
    "#tokenized_test_data['input_ids'] # columns are not tensors, will need to convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a0f874f-f466-4c81-a3dd-1498f54dc0cd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1741246438118,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "tokenized_test_data_with_direct_tokenizer_call = tokenizer(test_data['Review'], \n                     return_tensors=\"pt\", \n                     #padding=True,\n                     padding='max_length',\n                     truncation=True, \n                     max_length=200)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "#using direct tokenizer call (output not used in the code below, just for illustrating the differences)\n",
    "\n",
    "tokenized_test_data_with_direct_tokenizer_call = tokenizer(test_data['Review'], \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True,\n",
    "                     #padding='max_length',\n",
    "                     truncation=True, \n",
    "                     max_length=200)\n",
    "\n",
    "print(type(tokenized_test_data_with_direct_tokenizer_call))\n",
    "\n",
    "#tokenized_test_data_with_direct_tokenizer_call.input_ids # the columns are tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d7fb1-d755-4a29-adf6-b99c3bb6d1d4",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d7d9ab1-8f9a-4328-9192-189eff56b1a0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 765,
    "lastExecutedAt": 1741246438987,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Assuming the model is already on the correct device (CPU or GPU)\ndevice = model.device  # Get the device where the model is located\n\n# Ensure tokenized_test_data['input_ids'] and tokenized_test_data['attention_mask'] are tensors\ninput_ids = torch.tensor(tokenized_test_data['input_ids']).to(device)\nattention_mask = torch.tensor(tokenized_test_data['attention_mask']).to(device)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Disable gradient computation to save memory\nwith torch.no_grad():\n    predicted_labels_AM = model(input_ids=input_ids, attention_mask=attention_mask)\n\n# Check the model output structure\nprint(predicted_labels_AM)\n\n# Extract logits from the model output\nlogits = predicted_labels_AM.logits  # This is usually a tensor of shape (batch_size, num_labels)\n\n# Get the predicted labels by taking the argmax (class with the highest score)\npredictions_AM = torch.argmax(logits, dim=1).tolist()\n\n# Now, predicted_labels contains the predicted class indices for each example\nprint(predictions_AM)\n# \t0: Negative\n#\t1: Positive",
    "outputsMetadata": {
     "0": {
      "height": 143,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.5634,  2.5600],\n",
      "        [-0.8886,  0.9725],\n",
      "        [-3.6580,  3.8730],\n",
      "        [ 2.7306, -2.3037],\n",
      "        [-1.5670,  1.6940]]), hidden_states=None, attentions=None)\n",
      "[1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Assuming the model is already on the correct device (CPU or GPU)\n",
    "device = model.device  # Get the device where the model is located\n",
    "\n",
    "# Ensure tokenized_test_data['input_ids'] and tokenized_test_data['attention_mask'] are tensors\n",
    "input_ids = torch.tensor(tokenized_test_data['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(tokenized_test_data['attention_mask']).to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation to save memory\n",
    "with torch.no_grad():\n",
    "    predicted_labels_AM = model(input_ids=input_ids, attention_mask=attention_mask) #no need to use generate since not generating\n",
    "\n",
    "# Check the model output structure\n",
    "print(predicted_labels_AM)\n",
    "\n",
    "# Extract logits from the model output\n",
    "logits = predicted_labels_AM.logits  # This is usually a tensor of shape (batch_size, num_labels)\n",
    "\n",
    "# Get the predicted labels by taking the argmax (class with the highest score)\n",
    "predictions_AM = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "# Now, predicted_labels contains the predicted class indices for each example\n",
    "print(predictions_AM)\n",
    "# \t0: Negative\n",
    "#\t1: Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf7aeb-138c-48ad-af9e-18b63a28795a",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcdb2176-dc05-4794-ad4c-1f894ccb7200",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 167,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1741246439154,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "accuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b213cef3-9ea7-4e15-8e83-91264b5adec5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1741246439206,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "real_labels = [1 if data[\"Class\"] == \"POSITIVE\" else 0 for data in test_data]\nreal_labels"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Labels: [1, 0, 1, 0, 1]\n",
      "Predicted Labels: [1, 1, 1, 0, 1]\n",
      "{'accuracy': 0.8}\n",
      "{'f1': 0.8571428571428571}\n"
     ]
    }
   ],
   "source": [
    "# my solution\n",
    "\n",
    "real_labels = [1 if data[\"Class\"] == \"POSITIVE\" else 0 for data in test_data]\n",
    "print(f\"Real Labels: {real_labels}\")\n",
    "print(f\"Predicted Labels: {predictions}\")\n",
    "\n",
    "accuracy_result_compute = accuracy.compute(references=real_labels, predictions=predictions)\n",
    "accuracy_result = accuracy_result_compute['accuracy']\n",
    "print(accuracy_result_compute)\n",
    "f1_result_compute = f1.compute(references=real_labels, predictions=predictions)\n",
    "f1_result = f1_result_compute['f1']\n",
    "print(f1_result_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d51ada17-38cf-4fbb-8a2d-05eb4bb5b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Labels: [1, 0, 1, 0, 1]\n",
      "Predicted Labels: [1, 1, 1, 0, 1]\n",
      "Accuracy: 0.8\n",
      "F1 result: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "## datacamp solution\n",
    "\n",
    "# Map categorical sentiment labels into integer labels\n",
    "references_dc = [1 if label == \"POSITIVE\" else 0 for label in real_labels_dc]\n",
    "predictions_dc = [1 if label['label'] == \"POSITIVE\" else 0 for label in predicted_labels_dc]\n",
    "\n",
    "print(f\"Real Labels: {references_dc}\")\n",
    "print(f\"Predicted Labels: {predictions_dc}\")\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_result_dict_dc = accuracy.compute(references=references_dc, predictions=predictions_dc)\n",
    "accuracy_result_dc = accuracy_result_dict_dc['accuracy']\n",
    "f1_result_dict_dc = f1.compute(references=references_dc, predictions=predictions_dc)\n",
    "f1_result_dc = f1_result_dict_dc['f1']\n",
    "print(f\"Accuracy: {accuracy_result_dc}\")\n",
    "print(f\"F1 result: {f1_result_dc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7898f2f-4d9f-41e7-ab8a-ae439c7e483a",
   "metadata": {},
   "source": [
    "## **TEXT TRANSLATION MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76a95c-4a72-48a4-9551-80b40b91c37c",
   "metadata": {},
   "source": [
    "### USING PIPELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfc7bd00-50bd-4f4d-8cc7-0053e19fa48a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3192,
    "lastExecutedAt": 1741246442498,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "translator = pipeline(task=\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n\ntranslated_review_out = translator(english_data, clean_up_tokenization_spaces=True)\n\nprint(translated_review_out)\n\ntranslated_review = translated_review_out[0]['translation_text']\n\nprint(translated_review)\n# for sentence in translated_review_out:\n#     translated_review = sentence['translation_text']\n#     print(translated_review)\n# #print(output[0][\"translation_text\"])",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal.\n"
     ]
    }
   ],
   "source": [
    "# my solution usine pipeline\n",
    "\n",
    "# \tâ€¢\tHere, truncation happens manually before passing the text to the model.\n",
    "\t# â€¢\tThe model never sees the truncated part.\n",
    "\t# â€¢\tIf important context is cut off, the translation might lose meaning or fluency.\n",
    "\n",
    "# first 2 sentences of the 1st review\n",
    "english_data = ['I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use.']\n",
    "\n",
    "translator = pipeline(task=\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "translated_review_out = translator(english_data, clean_up_tokenization_spaces=True)\n",
    "\n",
    "translated_review = translated_review_out[0]['translation_text']\n",
    "print(translated_review)\n",
    "# for sentence in translated_review_out:\n",
    "#     translated_review = sentence['translation_text']\n",
    "#     print(translated_review)\n",
    "# #print(output[0][\"translation_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43404863-c0fe-4a10-bad7-68960e9acc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your input_length: 365 is bigger than 0.9 * max_length: 27. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model translation:\n",
      "Estoy muy satisfecho con mi 2014 Nissan NV SL. Uso esta furgoneta para mis entregas de negocios y uso personal.\n"
     ]
    }
   ],
   "source": [
    "# datacamp solution using pipeline\n",
    "\n",
    "# Truncation Inside Translator\n",
    "# Here, truncation happens inside the translator() function.\n",
    "# \tâ€¢\tThe model processes the entire first_review and only outputs up to 27 tokens.\n",
    "# \tâ€¢\tThe model determines which part of the text is most important for translation based on its learned attention mechanism.\n",
    "\n",
    "# Best Practice: If possible, let the model handle truncation (max_length), unless youâ€™re sure the input is too long for the model. ðŸš€\n",
    "\n",
    "\n",
    "# Load translation LLM into a pipeline and translate car review\n",
    "first_review = reviews_dc[0]\n",
    "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "# max_length=27 to limit to only the first 2 sentences of the 1st review\n",
    "translated_review_dc = translator(first_review, max_length=27)[0]['translation_text']\n",
    "print(f\"Model translation:\\n{translated_review_dc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25676b28-5c9e-4820-b417-dd5671b0f2d4",
   "metadata": {},
   "source": [
    "### USING AUTOMODEL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "879a13f3-d730-46c0-ab9f-67808503ae74",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3090,
    "lastExecutedAt": 1741246445589,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\ntranslation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n\nenglish_input = translation_tokenizer(english_data, \n                     return_tensors=\"pt\", \n                     padding=True,\n                     truncation=True, \n                     max_length=100)\n\n# Set model to evaluation mode\ntranslation_model.eval()\n\n# Disable gradient computation to save memory\nwith torch.no_grad():\n    translated_review_out = translation_model.generate(**english_input)\n    \n# Decode the translated tokens to get the translated text\ntranslated_review_decoded = [translation_tokenizer.decode(tokens, skip_special_tokens=True) for tokens in translated_review_out]\n\nprint(translated_review_decoded)\n\ntranslated_review = translated_review_decoded[0]\nprint(translated_review)\n# Print the translated sentences\n# for sentence in translated_review_out:\n#     translated_review = sentence\n#     print(translated_review)",
    "outputsMetadata": {
     "0": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumitsinha/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65000,  1686,   239, 22669,    29,   155, 40906,   716,  1239,   175,\n",
      "           399,  8356,  9652,   135, 26327,    24,  1071,  3678,     9,     4,\n",
      "          4553,    11,   694,   429,     3,     0]])\n",
      "Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal.\n"
     ]
    }
   ],
   "source": [
    "translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "english_input = translation_tokenizer(english_data, \n",
    "                     return_tensors=\"pt\", \n",
    "                     padding=True,\n",
    "                     truncation=True, \n",
    "                     max_length=100)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "translation_model.eval()\n",
    "\n",
    "# Disable gradient computation to save memory\n",
    "with torch.no_grad():\n",
    "    translated_review_out = translation_model.generate(**english_input) # using generate to generate translation\n",
    "\n",
    "print(translated_review_out)\n",
    "    \n",
    "# Decode the translated tokens to get the translated text\n",
    "translated_review_decoded = [translation_tokenizer.decode(tokens, skip_special_tokens=True) for tokens in translated_review_out]\n",
    "\n",
    "#print(translated_review_decoded)\n",
    "\n",
    "translated_review = translated_review_decoded[0]\n",
    "print(translated_review)\n",
    "# Print the translated sentences\n",
    "# for sentence in translated_review_out:\n",
    "#     translated_review = sentence\n",
    "#     print(translated_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb8fae-b171-4a54-9d50-815e090123cc",
   "metadata": {},
   "source": [
    "### Text Translation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56f41a41-39e0-48be-8645-b916138774e9",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1741246445638,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Open and read the file into a list\nwith open(\"data/reference_translations.txt\", \"r\") as file:\n    reference_translations = file.readlines()\n\n# Strip newlines and any extra spaces\nreference_translations = [line.strip() for line in reference_translations] # each line is a list\n\n# Print the list\nprint(reference_translations)",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish translation references:\n",
      "['Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.', 'Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.']\n",
      "English Text:\n",
      "Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal.\n"
     ]
    }
   ],
   "source": [
    "# Open and read the file into a list\n",
    "with open(\"data/reference_translations.txt\", \"r\") as file:\n",
    "    reference_translations = file.readlines()\n",
    "\n",
    "# Strip newlines and any extra spaces\n",
    "reference_translations = [line.strip() for line in reference_translations] # each line is a list\n",
    "\n",
    "# Print the list\n",
    "print(f\"Spanish translation references:\\n{reference_translations}\")\n",
    "\n",
    "print(f\"English Text:\\n{translated_review}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa13ba2c-385c-4869-9904-03a2ba241128",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1217,
    "lastExecutedAt": 1741246446906,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "bleu = evaluate.load(\"bleu\")\nbleu_score = bleu.compute(predictions=[translated_review], references=[reference_translations])\nprint(bleu_score)",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.7794483794144497, 'precisions': [0.9090909090909091, 0.8571428571428571, 0.75, 0.631578947368421], 'brevity_penalty': 1.0, 'length_ratio': 1.0476190476190477, 'translation_length': 22, 'reference_length': 21}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=[translated_review], references=[reference_translations])\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff04eab6-dd45-4106-aebf-437dacb2bb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6022774485691839\n"
     ]
    }
   ],
   "source": [
    "# datacamp solution (different blue score from my solution because the datacamp solution translated text is slight different)\n",
    "\n",
    "# Load and calculate BLEU score metric\n",
    "bleu_score = bleu.compute(predictions=[translated_review_dc], references=[reference_translations])\n",
    "print(bleu_score['bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a93487-dd16-462e-84bd-efec0500ba87",
   "metadata": {},
   "source": [
    "## **EXTRACTIVE Q&A MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1009ed3c-693b-45f8-950c-609296824620",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1741246446958,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "test_data['Review'][1]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace46cb-f95c-4ddf-b35f-b17630f7ff3f",
   "metadata": {},
   "source": [
    "### USING PIPELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5abd0a82-a5d7-4dbb-864b-521c0dfc931e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 869,
    "lastExecutedAt": 1741246447827,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "question = \"What did he like about the brand?\"\ncontext = test_data['Review'][1]\n\n# Define the appropriate model\nqa = pipeline(task=\"question-answering\", model='deepset/minilm-uncased-squad2')\n\ninput_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n\noutput = qa({\"context\": context, \"question\": question}, max_length=150)\nanswer = output['answer']",
    "outputsMetadata": {
     "5": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "# Using Pipeline - this is actually extractive Q&A Model\n",
    "\n",
    "question = \"What did he like about the brand?\"\n",
    "context = test_data['Review'][1]\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model='deepset/minilm-uncased-squad2')\n",
    "\n",
    "output = qa(question=question, context=context)\n",
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43c20c-f776-42e5-be01-e822bae29cbb",
   "metadata": {},
   "source": [
    "### USING AUTOMODEL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd61928b-5367-4368-ae38-1252e0d5a619",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1231,
    "lastExecutedAt": 1741246449109,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load model and tokenizer\nQAmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\nQAtokenizer = AutoTokenizer.from_pretrained('deepset/minilm-uncased-squad2')\n\n# Define question and context\nquestion = \"What did he like about the brand?\"\ncontext = test_data['Review'][1]\n\n# Tokenize input\ninputs = QAtokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True)\n\n# Perform inference\nQAmodel.eval()\nwith torch.no_grad():\n    outputs = QAmodel(**inputs)\n\n# Extract answer span\nstart_scores = outputs.start_logits\nend_scores = outputs.end_logits\nstart_index = torch.argmax(start_scores)\nend_index = torch.argmax(end_scores) + 1  # +1 to include the last token\n\n# Decode answer\nanswer = QAtokenizer.convert_tokens_to_string(QAtokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index]))\n\nprint(\"Answer:\", answer)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     },
     "1": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 2.2762, -6.2343, -6.0321, -6.0678, -6.0882, -6.0398, -6.0916, -6.3243,\n",
      "         -6.6169,  2.2762, -4.1784, -5.3074, -6.0259, -5.0759, -6.3563, -4.7436,\n",
      "         -6.3526, -6.4298, -5.7884, -5.7856, -5.1914, -6.4162, -4.9813, -5.8880,\n",
      "         -6.1990, -6.5654, -5.0210, -6.2009, -6.6289, -6.4944, -5.6861, -6.2675,\n",
      "         -6.1552, -6.4367, -6.4137, -5.1342, -5.3643, -6.2586, -5.5127, -6.2846,\n",
      "         -6.5086, -6.5867, -5.4555, -5.4227, -6.3117, -6.0545, -6.1566, -6.4791,\n",
      "         -6.6384, -6.8284, -6.2884, -5.4548, -5.0252, -6.0445, -6.5075, -5.9411,\n",
      "         -6.0409, -6.8019, -6.3508, -5.8416, -6.7474, -6.7375, -6.0882, -6.3069,\n",
      "         -5.2681, -5.4771, -6.5445, -6.5645, -6.5778, -6.5040, -5.7578, -5.9364,\n",
      "         -6.3930, -6.5072, -6.7121, -6.6489, -5.7286, -6.0741, -6.1914, -5.8883,\n",
      "         -6.2878, -6.8106, -6.4051, -6.0894, -6.1538, -6.4427, -6.1129, -6.5051,\n",
      "         -6.7101, -6.6582, -3.8438, -4.4204, -5.9236, -5.8944, -5.1940, -6.0101,\n",
      "         -5.1710, -6.7102, -5.3207, -6.3170, -6.3437, -6.6149, -6.3897, -6.2319,\n",
      "         -6.2741, -6.7939, -6.3751, -3.2043, -4.9956, -5.6109, -5.8733, -5.5840,\n",
      "         -4.4363, -6.3063, -6.0574, -4.4409, -5.7468, -5.0566, -5.6441, -5.5577,\n",
      "         -4.8432, -5.9208, -5.9335, -5.6904, -5.5082, -6.2614, -4.3558, -0.1342,\n",
      "         -4.2098,  1.3126, -0.0211, -2.1085,  0.5149, -0.6549, -1.4021, -5.1147,\n",
      "         -4.0444,  1.1624,  6.5843, -0.4191, -4.1999,  0.0190, -5.4061, -3.7833,\n",
      "         -3.0379, -2.7572, -4.7838, -5.2645, -5.0845, -6.1400, -6.0869, -6.1631,\n",
      "         -6.0246, -5.2798, -6.0814, -6.0982, -6.5720, -5.8292, -6.1484, -6.5104,\n",
      "         -6.3216, -4.4269, -3.7098, -5.9750, -2.5570, -6.1964, -5.7527, -6.2070,\n",
      "         -6.1268, -5.2331, -6.1594, -5.1747, -6.8336, -6.0148, -4.8801, -6.1285,\n",
      "         -6.1785, -6.4428, -6.5086, -6.2561, -6.7754, -6.7826,  2.2762]]), end_logits=tensor([[ 1.7217, -5.5849, -5.7638, -5.7849, -5.7597, -5.9993, -6.0658, -5.7188,\n",
      "         -5.3955,  1.7217, -6.4926, -5.5980, -6.0494, -3.9630, -3.8977, -6.5789,\n",
      "         -5.9050, -5.7757, -5.6061, -6.1973, -5.4223, -6.0774, -6.5524, -6.3096,\n",
      "         -3.7424, -4.1772, -6.8504, -6.1628, -5.5142, -5.6960, -6.6004, -6.4772,\n",
      "         -6.4225, -5.5627, -5.8090, -6.6598, -6.0527, -6.1480, -6.3114, -6.1945,\n",
      "         -4.6297, -4.6909, -6.5800, -5.8811, -5.8286, -6.1960, -6.3269, -5.9047,\n",
      "         -5.5221, -5.1942, -6.2089, -6.5062, -4.7839, -6.3639, -5.9842, -6.4674,\n",
      "         -6.3350, -5.2968, -6.2943, -6.3907, -5.1605, -5.3773, -6.3443, -6.1900,\n",
      "         -6.7503, -6.4986, -5.6607, -5.6428, -5.8623, -6.0834, -6.6467, -6.1517,\n",
      "         -6.2530, -5.6364, -5.6786, -5.2648, -6.5790, -6.2122, -6.3331, -6.5364,\n",
      "         -5.1746, -5.4133, -6.2698, -6.5477, -6.2277, -6.2842, -6.4320, -6.1782,\n",
      "         -5.4090, -5.1533, -6.2739, -4.5883, -6.2048, -5.5660, -6.5231, -6.2561,\n",
      "         -4.0107, -4.7839, -6.1106, -6.0624, -6.2059, -5.4922, -5.8332, -6.1128,\n",
      "         -6.2036, -4.6374, -5.0095, -6.3997, -6.4959, -6.2846, -4.9585, -6.5286,\n",
      "         -5.9795, -4.9209, -3.8251, -6.6017, -6.3675, -6.3518, -6.1716, -3.9460,\n",
      "         -5.4405, -6.1387, -5.3452, -6.0140, -6.2078, -3.3367, -2.8690, -2.9335,\n",
      "         -4.3813, -3.2139, -3.7768, -3.6477, -4.3307, -5.9965, -4.2802, -5.3511,\n",
      "         -0.8512, -1.1055,  0.1324,  2.7504, -1.1518,  4.6218,  0.1214,  2.6589,\n",
      "          4.1670, -4.1144, -5.8860, -6.2337, -5.9412, -5.8832, -5.9582, -1.9777,\n",
      "         -2.0568, -6.7672, -6.4071, -5.4718, -5.6850, -5.9542, -6.3319, -5.8220,\n",
      "         -5.9721, -6.8000, -6.4552, -6.4716, -2.5971, -6.2833, -6.5310, -5.0169,\n",
      "         -6.2512, -6.3676, -6.3587, -4.4565, -4.9604, -6.3348, -6.5119, -6.0018,\n",
      "         -6.1153, -5.8433, -6.1410, -5.7085, -4.6369, -4.4519,  1.7217]]), hidden_states=None, attentions=None)\n",
      "tensor(138) tensor(142)\n",
      "Answer: ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "# My solution using AutoModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "QAmodel = AutoModelForQuestionAnswering.from_pretrained('deepset/minilm-uncased-squad2')\n",
    "QAtokenizer = AutoTokenizer.from_pretrained('deepset/minilm-uncased-squad2')\n",
    "\n",
    "# Define question and context\n",
    "question = \"What did he like about the brand?\"\n",
    "context = test_data['Review'][1]\n",
    "\n",
    "# Tokenize input\n",
    "inputs = QAtokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "QAmodel.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = QAmodel(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "# The model returns two sets of scores:\n",
    "# \tâ€¢\tstart_logits: Scores indicating the probability of each token being the start of the answer.\n",
    "# \tâ€¢\tend_logits: Scores indicating the probability of each token being the end of the answer.\n",
    "# These scores are logits (raw scores before applying softmax).\n",
    "\n",
    "# Extract answer span\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# â€¢\ttorch.argmax(start_scores): Finds the index of the token most likely to be the start of the answer.\n",
    "# â€¢\ttorch.argmax(end_scores): Finds the index of the token most likely to be the end of the answer.\n",
    "# â€¢\t+1: Since Python slicing is exclusive on the right, adding 1 ensures we include the last token.\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores) + 1  # +1 to include the last token\n",
    "\n",
    "print(start_index, end_index)\n",
    "\n",
    "# Decode answer\n",
    "# â€¢\tinputs[\"input_ids\"][0]: Retrieves the token IDs corresponding to the input text.\n",
    "# â€¢\tconvert_ids_to_tokens(...): Converts token IDs back into words (subword tokens).\n",
    "# â€¢\tconvert_tokens_to_string(...): Joins the tokens into a human-readable sentence.\n",
    "#answer = QAtokenizer.convert_tokens_to_string(QAtokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index]))\n",
    "#OR\n",
    "answer = QAtokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index])\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf9016f9-9dbe-4b16-8ee0-6fcb6b0a8e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Answer:  ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "# datacamp solution using AutoModel - similar to my solution\n",
    "\n",
    "# Instantiate model and tokenizer\n",
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "# Define context and question, and tokenize them\n",
    "context = reviews_dc[1]\n",
    "print(f\"Context:\\n{context}\")\n",
    "question = \"What did he like about the brand?\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference and extract answer from raw outputs\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode and show answer\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceeeecd-abb3-4cd6-b8ea-49fa1ec924bb",
   "metadata": {},
   "source": [
    "## TEXT SUMMARIZATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c31be972-0cc1-4226-87e5-490f8bea853c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1741246449158,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "test_data['Review'][4]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#last review\n",
    "#test_data['Review'][4]\n",
    "#OR\n",
    "test_data['Review'][-1]  # much better way of finding last review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e591212-1db0-4c2d-b3dc-a72bb63a9125",
   "metadata": {},
   "source": [
    "### USING PIPELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7d58d46-a9bc-4162-9f3f-d8a72013a997",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 20945,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1741246470103,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")\n\nsummarized_text = summarizer(test_data['Review'][4], min_length=50, max_length=50, clean_up_tokenization_spaces=True)\nprint(summarized_text[0][\"summary_text\"])",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     },
     "6": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your min_length=56 must be inferior than your max_length=51.\n",
      "/Users/sumitsinha/miniconda3/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:1432: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (51). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more.\n"
     ]
    }
   ],
   "source": [
    "# My solution using pipeline\n",
    "\n",
    "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summarized_text = summarizer(test_data['Review'][-1], max_length=51, clean_up_tokenization_spaces=True)\n",
    "print(summarized_text[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c148323-6cf9-436f-a779-efa56f70f974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text:\n",
      "the Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. I have hauled 12 bags of mulch in the back with the seats down and could have held more.\n"
     ]
    }
   ],
   "source": [
    "# datacamp solution using pipeline (uses a different model)\n",
    "\n",
    "# Get original text to summarize upon car review\n",
    "text_to_summarize = reviews_dc[-1]\n",
    "print(f\"Original text:\\n{text_to_summarize}\")\n",
    "\n",
    "# Load summarization pipeline and perform inference\n",
    "model_name = \"cnicu/t5-small-booksum\" \n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "outputs = summarizer(text_to_summarize, max_length=53)\n",
    "summarized_text = outputs[0]['summary_text']\n",
    "print(f\"Summarized text:\\n{summarized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dee011-cb99-4051-8a5d-acaeeb0a32df",
   "metadata": {},
   "source": [
    "### USING AUTOMODEL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ca514af-f908-49f2-b4a9-39040adbd6e1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10142,
    "lastExecutedAt": 1741246480246,
    "lastExecutedByKernel": "4a789019-c131-4881-ba69-6cbeb23c45f6",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load model and tokenizer\nmodel_name = \"facebook/bart-large-cnn\"\nSUMMARYtokenizer = AutoTokenizer.from_pretrained(model_name)\nSUMMARYmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Select the text to summarize\ninput_text = test_data['Review'][4]\n\n# Tokenize input\ninputs = SUMMARYtokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n\n# Generate summary\nSUMMARYmodel.eval()\nwith torch.no_grad():\n    summary_ids = SUMMARYmodel.generate(**inputs, min_length=50, max_length=50, length_penalty=2.0)\n\n# Decode summary\nsummarized_text = SUMMARYtokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\nprint(\"Summary:\", summarized_text)",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more.\n"
     ]
    }
   ],
   "source": [
    "# My solution using AutoModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "summarytokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summarymodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Select the text to summarize\n",
    "input_text = test_data['Review'][-1]\n",
    "\n",
    "# Tokenize input\n",
    "inputs = summarytokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summarymodel.eval()\n",
    "\n",
    "# The length_penalty parameter in generate() controls the preference for longer or shorter sequences in text generation.\n",
    "# length_penalty=2.0 encourages shorter outputs because a higher penalty (>1.0) discourages longer sequences.\n",
    "# â€¢\tIf length_penalty < 1.0 â†’ Favors longer outputs.\n",
    "# â€¢\tIf length_penalty > 1.0 â†’ Favors shorter outputs.\n",
    "# â€¢\tIf length_penalty = 1.0 â†’ No length penalty applied (default behavior).\n",
    "\n",
    "with torch.no_grad():\n",
    "    summary_ids = summarymodel.generate(**inputs, max_length=51, length_penalty=2.0)\n",
    "\n",
    "# Decode summary\n",
    "summarized_text = summarytokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(\"Summary:\", summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654fb70-5402-4126-b3b6-2cd83f101993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de0745-a688-4b80-98b3-b6c65cee39ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84919f-1f67-443d-8ec0-a87881a9d486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe1a1e-b229-49c7-ab5e-44ea797356ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c8b8c-8808-4af2-adfe-16da8046e169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15cf94-07c9-4390-95b3-209b905691ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f38c6fc-651b-455d-94e5-4f85d9282a4d",
   "metadata": {},
   "source": [
    "## TEXT GENERATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561427ef-ec7c-41aa-80ff-7a13db594e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gion neighborhood in Kyoto is famous for its art and its colorful landscape. It's not the only place to have a family or a friend in Gion, Japan. In the city's northern suburbs, it is a unique and unique place to have a family or a friend anywhere. The name Gion refers to Gionshi Shiii in Japan. In the city's central city, it's the second-highest ranking family in the country. Other cities have the highest\n"
     ]
    }
   ],
   "source": [
    "# Using Pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n",
    "prompt = \"The Gion neighborhood in Kyoto is famous for\"\n",
    "output = generator(prompt, max_length=100, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(\" \".join(output[0][\"generated_text\"].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18930728-0512-4c4f-b238-9be4d3dd1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,   402,   295,  6232,   287, 36298,   318,  5863,   329]])\n",
      "tensor([[  464,   402,   295,  6232,   287, 36298,   318,  5863,   329,   663,\n",
      "          1029,    12, 17163,    11,  1029,    12, 17163,    11,   290,  1029,\n",
      "            12, 17163,  6832,    13,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "           198,   198,   198,   198,   198,   198,   198,   198,   198,   198]])\n",
      "The Gion neighborhood in Kyoto is famous for its high-rise, high-rise, and high-rise buildings.\n"
     ]
    }
   ],
   "source": [
    "# Using AutoModel - takes too much memory - times out without torch_dtype=torch.float16\n",
    "# torch.float16: load the model in half-precision (fp16). This significantly reduces memory consumption.\n",
    "# But not very good text generations\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# Define prompt\n",
    "prompt = \"The Gion neighborhood in Kyoto is famous for\"\n",
    "#input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# OR\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "print(output_ids)   \n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdddbb70-1fe5-473c-9d0d-169f232d363f",
   "metadata": {},
   "source": [
    "## GENERATIVE Q&A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62dd9689-df59-471a-86c8-0d0fb801a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "# Using Pipeline - CORRECT extractive Q&A Model with using deepset/minilm-uncased-squad2\n",
    "\n",
    "question = \"What did he like about the brand?\"\n",
    "context = test_data['Review'][1]\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model='deepset/minilm-uncased-squad2')\n",
    "\n",
    "output = qa(question=question, context=context)\n",
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34dc0ef0-afad-4fa9-99bc-1493563f7b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rest of issues found.\n"
     ]
    }
   ],
   "source": [
    "# Using pipeline - CORRECT generative Q&A Model with using gpt2\n",
    "\n",
    "question = \"What did he like about the brand?\"\n",
    "context = test_data['Review'][1]\n",
    "\n",
    "# Define the appropriate model\n",
    "qa = pipeline(task=\"question-answering\", model='gpt2')\n",
    "\n",
    "input_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "output = qa({\"context\": context, \"question\": question}, max_length=150)\n",
    "answer = output['answer']\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e835babd-ae04-430b-80cc-f59b3bee6004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I like the way the company is handling the situation. I think the problem is that the dealership is not doing enough to address the problems. They are not taking the time to fix the cars. There are a lot of problems with the vehicles, so\n"
     ]
    }
   ],
   "source": [
    "# Using AutoModel APproach for generative QA\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the question and context\n",
    "question = \"What did he like about the brand?\"\n",
    "context = test_data['Review'][1]\n",
    "\n",
    "# Load the model and tokenizer (using GPT-2 for generative Q&A)\n",
    "model_name = \"gpt2\"  # Or use a larger model like \"gpt-3.5-turbo\" if available\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad_token to be the eos_token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare the input text combining question and context\n",
    "input_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n",
    "\n",
    "# Generate the answer (sampling or beam search can be added for diversity)\n",
    "# max_new_tokens ensures that the model generates no more than the specified number of tokens.\n",
    "# num_return_sequences=1: This specifies the number of different sequences the model should generate for the given input. \n",
    "# In this case, it will generate 1 sequence (one possible answer).\n",
    "# If you set it to num_return_sequences=3, the model will generate 3 distinct answers.\n",
    "# no_repeat_ngram_size=2: you are asking the model not to repeat any bigrams (two consecutive tokens) during the generation process.\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs['input_ids'], max_new_tokens=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode the generated answer\n",
    "generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the answer part (after \"Answer:\")\n",
    "answer_start = generated_answer.find(\"Answer:\") + len(\"Answer:\")\n",
    "answer = generated_answer[answer_start:].strip()\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98936ce-a006-4bd3-a93a-c14c50972821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3.11 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
