{"cells":[{"source":"![servicedesk](servicedesk.png)\n\nCleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.","metadata":{"executionCancelledAt":null,"executionTime":165,"lastExecutedAt":1707667023665,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."},"id":"e5870ae0-6165-459e-9c40-0f282883be7b","cell_type":"markdown"},{"source":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1742435128560,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd"},"id":"2fa90b61-0244-4236-aa93-e33a7a088eec","cell_type":"code","execution_count":59,"outputs":[]},{"source":"nltk.download('punkt')","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1742435128623,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nltk.download('punkt')","outputsMetadata":{"0":{"height":80,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd"},"id":"37a51a81-1301-4a80-b8c6-716faaff4c5c","cell_type":"code","execution_count":60,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":60}]},{"source":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')","metadata":{"executionCancelledAt":null,"executionTime":123,"lastExecutedAt":1742435128746,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')"},"id":"e1b12eaf-e55c-422c-94a2-b0197c465a1b","cell_type":"code","execution_count":61,"outputs":[]},{"source":"# print first 20 elements from the words list\n# these are our vocab words - 10146 of them\nprint(len(words))\nprint(words[0:20])","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1742435128797,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# print first 20 elements from the words list\n# these are our vocab words - 10146 of them\nprint(len(words))\nprint(words[0:20])","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"6d1cfc7a-e8c9-4220-a90f-fa544c47956e","outputs":[{"output_type":"stream","name":"stdout","text":"10146\n['_PAD', '_UNK', 'the', '.', 'i', 'to', ',', 'and', 'a', 'my', 'that', 'of', 'was', 'in', 'on', 'they', 'for', 'me', 'not', 'this']\n"}],"execution_count":62},{"source":"# print first 20 elements from the text list\n# these are tokenized sentences - 5000 of them\nprint(len(text))\nprint(text[0:2])","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435128845,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# print first 20 elements from the text list\n# these are tokenized sentences - 5000 of them\nprint(len(text))\nprint(text[0:2])","outputsMetadata":{"0":{"height":374,"type":"stream"}}},"cell_type":"code","id":"7be8a656-6007-46ba-bed8-277c2b8f25fa","outputs":[{"output_type":"stream","name":"stdout","text":"5000\n[['i', 'called', 'because', 'i', 'have', 'been', 'receiving', '7', 'to', '8', 'calls', 'a', 'day', 'from', 'them', 'regarding', 'a', 'debt', 'and', 'the', 'representative', 'called', 'me', 'a', 'liar', 'after', 'i', 'asked', 'about', 'settling', 'my', 'account'], ['this', 'call', 'took', 'place', 'at', 'around', 'noon', ',', 'i', 'had', 'noticed', 'that', 'there', 'were', 'some', 'unusual', 'activities', 'on', 'my', 'credit', 'report', '.', 'i', 'called', 'midland', 'funding', 'to', 'find', 'out', 'why', 'i', 'had', '3', 'chargeoff', 'that', 'were', 'not', 'there', 'before', '.', 'after', 'a', 'few', 'rings', ',', 'i', 'was', 'greeted', 'with', 'hello', 'my', 'name', 'is', 'and', 'i', 'am', 'debt', 'collection', 'agent', '.', 'i', 'had', 'asked', 'numerous', 'times', 'as', 'to', 'his', 'full', 'name', 'without', 'any', 'luck', '.', 'he', 'kept', 'interrupting', 'me', 'and', 'saying', 'i', 'owed', 'this', 'much', 'and', 'i', 'have', 'to', 'pay', 'in', 'full', 'or', 'payment', '.', 'i', 'was', 'told', 'to', 'make', 'payments', 'of', 'dollars', 'a', 'month', 'until', 'the', 'debt', 'was', 'paid', 'off', '.', 'i', 'ask', 'if', 'he', 'could', 'eplain', 'how', 'it', 'got', 'on', 'my', 'name', '.', 'he', 'did', 'not', 'eplain', '.', 'all', 'he', 'said', 'was', 'its', 'under', 'my', 'name', '.', 'i', 'either', 'had', 'pay', 'or', 'the', 'collection', 'will', 'stay', 'on', 'for', '7', 'years', '.', 'he', 'added', 'that', 'if', 'i', 'want', 'the', 'calls', 'to', 'me', 'to', 'stop', 'i', 'had', 'to', 'agree', 'to', 'pay', '.', 'i', 'said', 'dont', 'i', 'have', 'the', 'right', 'to', 'ask', 'about', 'the', 'collections', 'and', 'how', 'it', 'came', 'about', '.', 'he', 'said', ',', 'dont', 'you', 'know', 'the', 'law', 'if', 'you', 'dont', 'pay', '.', 'you', 'job', 'and', 'wages', 'will', 'be', 'garnished', '.', 'he', 'ended', 'the', 'call', 'by', 'saying', 'call', 'us', 'back', 'when', 'you', 'have', 'funds', 'to', 'pay', '.', 'all', '3', 'collection', 'companies', 'below', 'to', 'one', 'entity', '.', 'midland', 'credit', 'management', 'midland', 'funding', 'midland', 'funding']]\n"}],"execution_count":63},{"source":"# print first 20 elements from the labels list\n# these are labels for each sentence - 5000 of them\nprint(len(labels))\nprint(labels[0:2])","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435128893,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# print first 20 elements from the labels list\n# these are labels for each sentence - 5000 of them\nprint(len(labels))\nprint(labels[0:2])","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"8f6e4616-589c-43bb-8773-1366a884e75a","outputs":[{"output_type":"stream","name":"stdout","text":"5000\n[2 2]\n"}],"execution_count":64},{"source":"print(np.unique(labels))\nnum_classes = len(np.unique(labels))","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435128941,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(np.unique(labels))\nnum_classes = len(np.unique(labels))","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"4b6adcf6-46cf-4c26-a9ce-fcd605b3f3fd","outputs":[{"output_type":"stream","name":"stdout","text":"[0 1 2 3 4]\n"}],"execution_count":65},{"source":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1742435128993,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}"},"cell_type":"code","id":"26aadbb9-7967-433e-b7c9-5096ccb60877","outputs":[],"execution_count":66},{"source":"# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]","metadata":{"executionCancelledAt":null,"executionTime":167,"lastExecutedAt":1742435129160,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]"},"cell_type":"code","id":"ecc51b44-1bfb-4e08-a6e5-f8d0d9ea55a8","outputs":[],"execution_count":67},{"source":"print(len(text))\nprint(text[0:2])","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742435129209,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(len(text))\nprint(text[0:2])","outputsMetadata":{"0":{"height":227,"type":"stream"}}},"cell_type":"code","id":"5a586154-e232-4dbc-b2d1-960b3430c7a3","outputs":[{"output_type":"stream","name":"stdout","text":"5000\n[[4, 62, 78, 4, 20, 48, 476, 779, 5, 953, 215, 8, 149, 26, 45, 316, 8, 81, 7, 2, 230, 62, 17, 8, 5025, 68, 4, 103, 95, 6772, 9, 23], [19, 89, 276, 445, 41, 441, 3766, 6, 4, 32, 577, 10, 88, 58, 217, 2952, 1608, 14, 9, 29, 118, 3, 4, 62, 1255, 927, 5, 312, 76, 154, 4, 32, 275, 7917, 10, 58, 18, 88, 191, 3, 68, 8, 382, 5471, 6, 4, 12, 7918, 22, 1325, 9, 152, 21, 7, 4, 67, 81, 197, 326, 3, 4, 32, 103, 574, 165, 31, 5, 218, 264, 152, 167, 66, 2556, 3, 73, 596, 0, 17, 7, 307, 4, 343, 19, 442, 7, 4, 20, 5, 85, 13, 264, 46, 43, 3, 4, 12, 49, 5, 137, 104, 11, 447, 8, 178, 206, 2, 81, 12, 109, 193, 3, 4, 378, 70, 73, 92, 645, 177, 24, 204, 14, 9, 152, 3, 73, 50, 18, 645, 3, 59, 73, 91, 12, 295, 225, 9, 152, 3, 4, 509, 32, 85, 46, 2, 197, 80, 1285, 14, 16, 779, 173, 3, 73, 604, 10, 70, 4, 180, 2, 215, 5, 17, 5, 392, 4, 32, 5, 1154, 5, 85, 3, 4, 91, 697, 4, 20, 2, 323, 5, 378, 95, 2, 539, 7, 177, 24, 462, 95, 3, 73, 91, 6, 697, 61, 169, 2, 360, 70, 61, 697, 85, 3, 61, 617, 7, 3137, 80, 33, 3138, 3, 73, 928, 2, 89, 40, 307, 89, 124, 69, 57, 61, 20, 110, 5, 85, 3, 59, 275, 197, 535, 666, 5, 111, 1756, 3, 1255, 29, 793, 1255, 927, 1255, 927]]\n"}],"execution_count":68},{"source":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)","metadata":{"executionCancelledAt":null,"executionTime":66,"lastExecutedAt":1742435129275,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)"},"id":"d630badb-23dd-4368-9a96-e2b478ad5cff","cell_type":"code","execution_count":69,"outputs":[]},{"source":"print(len(text))\nprint(text[0:2])","metadata":{"executionCancelledAt":null,"executionTime":45,"lastExecutedAt":1742435129321,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(len(text))\nprint(text[0:2])","outputsMetadata":{"0":{"height":206,"type":"stream"}}},"cell_type":"code","id":"cda58978-1b4d-470f-b621-676b8378fb43","outputs":[{"output_type":"stream","name":"stdout","text":"5000\n[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    4   62   78    4   20   48  476  779    5  953\n   215    8  149   26   45  316    8   81    7    2  230   62   17    8\n  5025   68    4  103   95 6772    9   23]\n [  19   89  276  445   41  441 3766    6    4   32  577   10   88   58\n   217 2952 1608   14    9   29  118    3    4   62 1255  927    5  312\n    76  154    4   32  275 7917   10   58   18   88  191    3   68    8\n   382 5471    6    4   12 7918   22 1325]]\n"}],"execution_count":70},{"source":"**Preparing Training & Testing Data Data**","metadata":{},"cell_type":"markdown","id":"9cea1590-3516-46f7-9188-783c6698a039"},{"source":"# Splitting dataset\n# here inputs: text and labels are lists\n# ouputs: train_text, test_text, train_label, test_label are numpy arrays\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435129369,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitting dataset\n# here inputs: text and labels are lists\n# ouputs: train_text, test_text, train_label, test_label are numpy arrays\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"f2654836-631f-415e-9922-5ab3bafaaafa","cell_type":"code","execution_count":71,"outputs":[]},{"source":"print(type(train_data))\nprint(len(train_data))\ntrain_data[0]","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435129417,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(type(train_data))\nprint(len(train_data))\ntrain_data[0]","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"33c9881f-ed36-47f7-a5e3-89aac79950bb","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'torch.utils.data.dataset.TensorDataset'>\n4000\n"},{"output_type":"execute_result","data":{"text/plain":"(tensor([   4,   32,    8,  162,  190,   23,   41,  124,   30,    6,   60,  117,\n          301,   51,   48,  198,    7,  909,    5,   33,  198,   90,    5,  275,\n            8,  149,    5, 1847,   17, 3695,    6, 1646,    6,    7, 1866,    6,\n            5,    2,  384,   11, 4383,   11, 1458, 1208, 2724, 3029,   26,    2,\n         1045, 1013]),\n tensor(0))"},"metadata":{},"execution_count":72}],"execution_count":72},{"source":"# You can achieve the same functionality of TensorDataset by creating a custom dataset using torch.utils.data.Dataset. Here’s how you can rewrite that line using a custom dataset class:\n\nfrom torch.utils.data import Dataset\n\nclass CustomTextDataset(Dataset):\n    def __init__(self, text_data, labels):\n        self.text_data = torch.from_numpy(text_data)  # Convert NumPy array to tensor\n        self.labels = torch.from_numpy(labels).long()  # Convert labels to tensor (long for classification)\n\n    def __len__(self):\n        return len(self.text_data)\n\n    def __getitem__(self, idx):\n        return self.text_data[idx], self.labels[idx]\n\n# Create dataset instance\ntrain_dataset = CustomTextDataset(train_text, train_label)\n\n# Key Differences from TensorDataset:\n# 1.More Flexibility - You can add data augmentation, preprocessing, or additional logic in __getitem__.\n# 2.Custom Processing - Allows dynamic modifications instead of a fixed TensorDataset.\n# 3.Scalability - Useful when working with large datasets where on-the-fly transformations are needed.","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435129465,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# You can achieve the same functionality of TensorDataset by creating a custom dataset using torch.utils.data.Dataset. Here’s how you can rewrite that line using a custom dataset class:\n\nfrom torch.utils.data import Dataset\n\nclass CustomTextDataset(Dataset):\n    def __init__(self, text_data, labels):\n        self.text_data = torch.from_numpy(text_data)  # Convert NumPy array to tensor\n        self.labels = torch.from_numpy(labels).long()  # Convert labels to tensor (long for classification)\n\n    def __len__(self):\n        return len(self.text_data)\n\n    def __getitem__(self, idx):\n        return self.text_data[idx], self.labels[idx]\n\n# Create dataset instance\ntrain_dataset = CustomTextDataset(train_text, train_label)\n\n# Key Differences from TensorDataset:\n# 1.More Flexibility - You can add data augmentation, preprocessing, or additional logic in __getitem__.\n# 2.Custom Processing - Allows dynamic modifications instead of a fixed TensorDataset.\n# 3.Scalability - Useful when working with large datasets where on-the-fly transformations are needed."},"cell_type":"code","id":"031fce0b-e1b8-440d-a251-320f5f1fc83c","outputs":[],"execution_count":73},{"source":"print(type(train_dataset))\nprint(len(train_dataset))\ntrain_dataset[0]","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1742435129518,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(type(train_dataset))\nprint(len(train_dataset))\ntrain_dataset[0]","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"f5b01af8-efaf-4ef2-a4ab-6959544c1b8e","outputs":[{"output_type":"stream","name":"stdout","text":"<class '__main__.CustomTextDataset'>\n4000\n"},{"output_type":"execute_result","data":{"text/plain":"(tensor([   4,   32,    8,  162,  190,   23,   41,  124,   30,    6,   60,  117,\n          301,   51,   48,  198,    7,  909,    5,   33,  198,   90,    5,  275,\n            8,  149,    5, 1847,   17, 3695,    6, 1646,    6,    7, 1866,    6,\n            5,    2,  384,   11, 4383,   11, 1458, 1208, 2724, 3029,   26,    2,\n         1045, 1013]),\n tensor(0))"},"metadata":{},"execution_count":74}],"execution_count":74},{"source":"batch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size) # shuffle false for test","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1742435129569,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"batch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"},"cell_type":"code","id":"71a4c6a0-6f1e-4cba-92ad-0282c4cf1d38","outputs":[],"execution_count":75},{"source":"#print(next(iter(train_loader))[0:2]) # this is printing the first 2 batches\nbatch = next(iter(train_loader))\ninputs, labels = batch  # Unpack\nprint(inputs[:2], labels[:2])  # this will now only print first 2 samples","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742435129617,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#print(next(iter(train_loader))[0:2]) # this is printing the first 2 batches\nbatch = next(iter(train_loader))\ninputs, labels = batch  # Unpack\nprint(inputs[:2], labels[:2])  # this will now only print first 2 samples","outputsMetadata":{"0":{"height":227,"type":"stream"}}},"cell_type":"code","id":"0ec6fad7-28bd-4508-8857-8df675db1ec3","outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[   4,  751,    8,   43,   22, 1324,    7, 2741,    8,   43,  593,   68,\n           41,  568, 5549,  215,   22,   45,    3,    4,   32,    5,  234,  131,\n            8,  913, 2266, 5366,    5,   86,   14,    2,  593,    3,    4,  390,\n            5,   45,   10,    4,  254,    2,  112,  174,    5,   33, 3167,    5,\n            2,  409],\n        [   4, 1702,   90,    8,  949,   81,   22,  132,  134,  441,    6,   27,\n           25, 3022,   28,    3,    4,   98,   32,  217,  519,    3,   13,    4,\n          172,  131,    8, 1575,  718,   81,  546,   79,    6,    5,  241,   76,\n            8,   43,  593,    3,    2,  519,   58,   18,  109,   13,  264,    6,\n           44,  187]]) tensor([4, 0])\n"}],"execution_count":76},{"source":"**Build a CNN classifier**","metadata":{},"cell_type":"markdown","id":"7b3fc9a6-3f72-4dea-b9eb-94ffd01fa5da"},{"source":"class TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, num_classes)\n        \n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim=2)\n        return self.fc(conved)\n        # •\tCrossEntropyLoss expects raw logits (not probabilities).\n\t    # •\tIf you’ve applied softmax before passing to CrossEntropyLoss, it will lead to incorrect gradients.\n\t    # •\tFix: Remove softmax in the model’s forward(), as CrossEntropyLoss already applies it internally.","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435129665,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, num_classes)\n        \n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim=2)\n        return self.fc(conved)"},"cell_type":"code","id":"18ada272-81b9-41bb-88f2-8f92568278ca","outputs":[],"execution_count":77},{"source":"**Train the CNN classifier on train_data**","metadata":{},"cell_type":"markdown","id":"96dadea9-9517-40ff-9634-628edd30881e"},{"source":"vocab_size = len(word2idx) + 1 \n# Adding +1 is done to include a special token that wasn’t in the original vocabulary mapping, such as:\n# •\tPadding token (<PAD>): Needed when using batch processing, so all sequences have the same length.\n# •\tUnknown token (<UNK>): Assigned to words not in the vocabulary during inference.\nprint(vocab_size)\nembed_dim = 64","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435129713,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"vocab_size = len(word2idx) + 1 \n# Adding +1 is done to include a special token that wasn’t in the original vocabulary mapping, such as:\n# •\tPadding token (<PAD>): Needed when using batch processing, so all sequences have the same length.\n# •\tUnknown token (<UNK>): Assigned to words not in the vocabulary during inference.\nprint(vocab_size)\nembed_dim = 64","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"5eadafd1-695e-43d2-82bc-86aebffe31c0","outputs":[{"output_type":"stream","name":"stdout","text":"10147\n"}],"execution_count":78},{"source":"model = TicketClassifier(vocab_size, embed_dim, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742435129761,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model = TicketClassifier(vocab_size, embed_dim, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)"},"cell_type":"code","id":"14cccf97-f34d-4e4f-b106-4ad134e8e1bc","outputs":[],"execution_count":79},{"source":"model.train()\nfor epoch in range(3):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        output = model(inputs)\n        # print(output.shape) # torch.Size([400, 5])\n        ## For multi-class classification, your model output should have shape (batch_size, num_classes), where each row contains logits for the classes.\n        # print(labels.shape) # torch.Size([400])\n        ## Your labels should have shape (batch_size,) with class indices (not one-hot encoded).\n        loss = criterion(output, labels)\n        ## If model output is missing the second dimension (num_classes), it won’t work for multi-class classification.\n        ## If labels are one-hot encoded (instead of class indices), CrossEntropyLoss will not work.\n        ### Expected: (batch_size,)\n\t    ### Wrong: (batch_size, num_classes)\n\t    ### Fix: Convert one-hot labels to class indices using torch.argmax(labels, dim=-1)\n        \n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {epoch+1}, Loss: {running_loss/num_processed}\")\n    # num_processed is 4000, len(train_loader) is only 10 so cant use that\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"67d0c220-b4fb-4a95-89c2-7d086038e124","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, Loss: 5.51960361917736e-07\nEpoch: 2, Loss: 4.874967344221659e-07\nEpoch: 3, Loss: 4.508773636189289e-07\n"}],"execution_count":90},{"source":"**Test & Evaluate the CNN Classifier**","metadata":{},"cell_type":"markdown","id":"7398b6fe-6434-4ad9-a9ca-9d26ed46d2fc"},{"source":"accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1742435132285,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)"},"cell_type":"code","id":"46980b8c-eba4-4909-a794-54cbd45d701c","outputs":[],"execution_count":81},{"source":"model.eval()\n\npredicted = []\ntrue_labels = []\n\naccuracy_metric.reset()\nprecision_metric.reset()\nrecall_metric.reset()\n\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(test_loader):\n        output = model(inputs)\n        # selects the class with the highest score, \n        # It’s needed because the model outputs raw logits, not class labels\n        cat = torch.argmax(output, dim=-1) \n        # Why Not Use softmax Instead?\n        ## softmax converts logits into probabilities, but it doesn’t change the ranking of class scores.\n        ## argmax already finds the most probable class directly from logits.\n        ## If we only need class labels (not confidence scores), argmax is simpler and faster than softmax.\n        predicted.extend(cat.tolist()) # store predictions: refer CNN pytorch pg10 for \"extend\" function use\n        true_labels.extend(labels.tolist()) # store labels for verifying accuracy\n        \n        accuracy_metric.update(cat, labels)\n        precision_metric.update(cat, labels)\n        recall_metric.update(cat, labels)\n    \naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1742438927017,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model.eval()\n\npredicted = []\ntrue_labels = []\n\naccuracy_metric.reset()\nprecision_metric.reset()\nrecall_metric.reset()\n\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(test_loader):\n        output = model(inputs)\n        # selects the class with the highest score, \n        # It’s needed because the model outputs raw logits, not class labels\n        cat = torch.argmax(output, dim=-1) \n        # Why Not Use softmax Instead?\n        ## softmax converts logits into probabilities, but it doesn’t change the ranking of class scores.\n        ## argmax already finds the most probable class directly from logits.\n        ## If we only need class labels (not confidence scores), argmax is simpler and faster than softmax.\n        predicted.extend(cat.tolist()) # store predictions: refer CNN pytorch pg10 for \"extend\" function use\n        true_labels.extend(labels.tolist()) # store labels for verifying accuracy\n        \n        accuracy_metric.update(cat, labels)\n        precision_metric.update(cat, labels)\n        recall_metric.update(cat, labels)\n    \naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"09899da6-a7d1-4852-abe9-daffa4c44d2e","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.796999990940094\nPrecision (per class): [0.7213930487632751, 0.75, 0.8133333325386047, 0.8087431788444519, 0.8866994976997375]\nRecall (per class): [0.7552083134651184, 0.74210524559021, 0.8472222089767456, 0.7708333134651184, 0.8571428656578064]\n"}],"execution_count":94},{"source":"# Compute manual accuracy for verification\ncorrect = sum(p == l for p, l in zip(predicted, true_labels)) \naccuracy_manual = correct / len(true_labels)\nprint(f\"Manual Accuracy: {accuracy_manual:.4f}\") # Should match torchmetrics accuracy","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1742439042992,"lastExecutedByKernel":"9ac0b5f6-efc2-4334-9da8-b45bb37536cd","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Compute manual accuracy for verification\ncorrect = sum(p == l for p, l in zip(predicted, true_labels)) \naccuracy_manual = correct / len(true_labels)\nprint(f\"Manual Accuracy: {accuracy_manual:.4f}\") # Should match torchmetrics accuracy","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"3bf0b397-0ff2-4f37-af66-7ee401a25a2a","outputs":[{"output_type":"stream","name":"stdout","text":"Manual Accuracy: 0.7970\n"}],"execution_count":95}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}